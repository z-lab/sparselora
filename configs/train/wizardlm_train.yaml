dataset: "WizardLMTeam/WizardLM_evol_instruct_70k"
add_special_tokens: False
append_concat_token: False
splits: "train_sft,test_sft"
per_device_train_batch_size: 2
max_seq_length: 2048
model_max_length: 2048
num_train_epochs: 1
learning_rate: 2e-5
lr_scheduler_type: cosine
weight_decay: 1e-4
warmup_ratio: 0
max_grad_norm: 1.0
eval_strategy: "no"
save_strategy: "no"
logging_steps: 1
peft: "lora"
bf16: true
lora_dropout: 0.0
ddp_find_unused_parameters: false
packing: True
dataset_text_field: "content"