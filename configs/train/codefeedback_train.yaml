dataset: "m-a-p/CodeFeedback-Filtered-Instruction"
per_device_train_batch_size: 6
max_seq_length: 1024
model_max_length: 1024
num_train_epochs: 1
learning_rate: 2e-5
lr_scheduler_type: cosine
warmup_ratio: 0.04
eval_strategy: "no"
save_strategy: "no"
logging_steps: 1
peft: "lora"
bf16: true
lora_dropout: 0.0
ddp_find_unused_parameters: false
